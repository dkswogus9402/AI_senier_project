{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DTML",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "naET952gglbM"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s11tfpwx69pZ"
      },
      "source": [
        "def min_max_value():\n",
        "  df = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "\n",
        "  value_columes = [\"배추_가격(원/kg)\",\"무_가격(원/kg)\", \"양파_가격(원/kg)\", \"건고추_가격(원/kg)\", \"마늘_가격(원/kg)\",\\\n",
        "          \"대파_가격(원/kg)\", \"얼갈이배추_가격(원/kg)\", \"양배추_가격(원/kg)\",\\\n",
        "          \"깻잎_가격(원/kg)\", \"시금치_가격(원/kg)\", \"미나리_가격(원/kg)\",\\\n",
        "          \"당근_가격(원/kg)\",  \"파프리카_가격(원/kg)\", \"새송이_가격(원/kg)\", \"팽이버섯_가격(원/kg)\", \"토마토_가격(원/kg)\",\\\n",
        "          \"청상추_가격(원/kg)\",\"백다다기_가격(원/kg)\", \"애호박_가격(원/kg)\",\\\n",
        "          \"캠벨얼리_가격(원/kg)\", \"샤인마스캇_가격(원/kg)\"]\n",
        "\n",
        "  for i in value_columes:\n",
        "    df = df[df[i] !=0 ]\n",
        "\n",
        "  max_value = []\n",
        "  min_value = []\n",
        "\n",
        "  for value in value_columes:\n",
        "    min_ = min(df[value])\n",
        "    min_value.append(min_)\n",
        "    max_ = max(df[value])\n",
        "    max_value.append(max_)\n",
        "\n",
        "  return min_value, max_value\n",
        "\n",
        "\n",
        "min_value, max_value = min_max_value()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jve3seUXRAXR",
        "outputId": "b214f123-73e5-41d1-b30b-ab69d2288d55"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "\n",
        "value_columes = [\"배추_가격(원/kg)\",\"무_가격(원/kg)\", \"양파_가격(원/kg)\", \"건고추_가격(원/kg)\", \"마늘_가격(원/kg)\",\\\n",
        "          \"대파_가격(원/kg)\", \"얼갈이배추_가격(원/kg)\", \"양배추_가격(원/kg)\",\\\n",
        "          \"깻잎_가격(원/kg)\", \"시금치_가격(원/kg)\", \"미나리_가격(원/kg)\",\\\n",
        "          \"당근_가격(원/kg)\",  \"파프리카_가격(원/kg)\", \"새송이_가격(원/kg)\", \"팽이버섯_가격(원/kg)\", \"토마토_가격(원/kg)\",\\\n",
        "          \"청상추_가격(원/kg)\",\"백다다기_가격(원/kg)\", \"애호박_가격(원/kg)\",\\\n",
        "          \"캠벨얼리_가격(원/kg)\", \"샤인마스캇_가격(원/kg)\"]\n",
        "\n",
        "for i in value_columes:\n",
        "  df = df[df[i] !=0 ]\n",
        "  if i == \"건고추_가격(원/kg)\":\n",
        "    df = df[df[i] <= 30000]\n",
        "    df = df[df[i] >= 1000] \n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "scale_columes = [\"배추_거래량(kg)\", \"배추_가격(원/kg)\",\"무_거래량(kg)\", \"무_가격(원/kg)\",\"양파_거래량(kg)\", \"양파_가격(원/kg)\",\"건고추_거래량(kg)\", \"건고추_가격(원/kg)\",\"마늘_거래량(kg)\", \"마늘_가격(원/kg)\",\\\n",
        "           \"대파_거래량(kg)\", \"대파_가격(원/kg)\",\"얼갈이배추_거래량(kg)\", \"얼갈이배추_가격(원/kg)\",\"양배추_거래량(kg)\", \"양배추_가격(원/kg)\",\\\n",
        "           \"깻잎_거래량(kg)\", \"깻잎_가격(원/kg)\",\"시금치_거래량(kg)\", \"시금치_가격(원/kg)\",\"미나리_거래량(kg)\", \"미나리_가격(원/kg)\",\\\n",
        "           \"당근_거래량(kg)\", \"당근_가격(원/kg)\",  \"파프리카_거래량(kg)\", \"파프리카_가격(원/kg)\",\"새송이_거래량(kg)\", \"새송이_가격(원/kg)\",\"팽이버섯_거래량(kg)\", \"팽이버섯_가격(원/kg)\", \"토마토_거래량(kg)\", \"토마토_가격(원/kg)\",\\\n",
        "           \"청상추_거래량(kg)\", \"청상추_가격(원/kg)\",\"백다다기_거래량(kg)\", \"백다다기_가격(원/kg)\",\"애호박_거래량(kg)\", \"애호박_가격(원/kg)\",\\\n",
        "           \"캠벨얼리_거래량(kg)\", \"캠벨얼리_가격(원/kg)\",\"샤인마스캇_거래량(kg)\", \"샤인마스캇_가격(원/kg)\"]\n",
        "\n",
        "\n",
        "scaled = scaler.fit_transform(df[scale_columes])\n",
        "df_ = pd.DataFrame(scaled, columns= scale_columes )\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "train, valid = train_test_split(df_, test_size=0.3, random_state= 0, shuffle=True)\n",
        "valid, test = train_test_split(valid, test_size=0.4, random_state=0, shuffle=False)\n",
        "\n",
        "print(\"----------------train, test 분리 후 shape 출력-------------------\")\n",
        "print(\"train : \", train.shape,\"validation :\", valid.shape)\n",
        "\n",
        "def windowed_dataset(series, window_size, batch_size, shuffle, asp):\n",
        "    #series = tf.expand_dims(series, axis=-1)\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(window_size + asp, shift=1, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(window_size + asp))\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(1000)\n",
        "    ds = ds.map(lambda w: (w[:-asp], w[-1][1::2])) # two, four\n",
        "    return ds.batch(batch_size).prefetch(1)\n",
        "\n",
        "WINDOW_SIZE= 50\n",
        "BATCH_SIZE= 10 # 데이터는 총 50개씩 랜덤으로 뽑음\n",
        "asp = 7\n",
        "train_data = windowed_dataset(train, WINDOW_SIZE, BATCH_SIZE, True, asp) \n",
        "valid_data = windowed_dataset(valid, WINDOW_SIZE, BATCH_SIZE, False, asp) \n",
        "test_data = windowed_dataset(test, WINDOW_SIZE, BATCH_SIZE, False, asp)\n",
        "\n",
        "for data in train_data.take(1):\n",
        "    print(f'데이터셋(X) 구성(batch_size, window_size, feature갯수): {data[0].shape}')\n",
        "    print(f'데이터셋(Y) 구성(batch_size, window_size, feature갯수): {data[1].shape}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------train, test 분리 후 shape 출력-------------------\n",
            "train :  (623, 42) validation : (160, 42)\n",
            "데이터셋(X) 구성(batch_size, window_size, feature갯수): (10, 50, 42)\n",
            "데이터셋(Y) 구성(batch_size, window_size, feature갯수): (10, 21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlAwVX3FwWae"
      },
      "source": [
        "# Global 데이터를 포함 할 경우 아닐 경우 # \n",
        "\n",
        "# 1. locals_context : 포함 안함\n",
        "# 2. locals_context : 포함함\n",
        "\n",
        "# 큰 특징 : global data를 포함하는 경우 총 sector의 수가 달라짐\n",
        "# ex) 입력 : 21개의 sector\n",
        "#     출력 : 20개의 sector\n",
        "\n",
        "# local_context class의 sector_num만 수정한다면 자동으로 바뀜\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHcnqoWWhkfK"
      },
      "source": [
        "class Context_Vector(tf.keras.layers.Layer):\n",
        "  def __init__(self):\n",
        "    super(Context_Vector, self).__init__()\n",
        "    self.FC1 = tf.keras.layers.Dense(2) \n",
        "    self.LSTM = tf.keras.layers.LSTM(128, return_sequences = True, return_state = True, dropout= 0.2) \n",
        "    self.LayerNorm = tf.keras.layers.LayerNormalization (axis = -1) # test \n",
        "\n",
        "  def Attention(self, query, key, value): # query : LSTM hidden, key, \n",
        "    scores = tf.matmul(query, key, transpose_b=True) # score\n",
        "    distribution = tf.nn.softmax(scores)\n",
        "    return tf.matmul(distribution, value)\n",
        "\n",
        "  def call(self, input): # batch, window_size, feature\n",
        "    FC1 = self.FC1(input)\n",
        "    LSTM_1, LSTM_hidden, LSTM_cell = self.LSTM(FC1) # batch, window_size, feature\n",
        "    #print(\"LSTM_output : \",LSTM_1) # batch, window_size, LSTM units\n",
        "    LSTM_hidden = tf.expand_dims(LSTM_hidden, 1)\n",
        "    #print(\"LSTM_hidden : \",LSTM_hidden)\n",
        "    Attention1 = self.Attention(LSTM_hidden, LSTM_1, LSTM_1) \n",
        "    #print(\"Attention output : \",Attention1)\n",
        "    LayerNorm = self.LayerNorm(Attention1)\n",
        "    return LayerNorm # (batch, 1, 128)\n",
        " \n",
        "class locals_context(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self,sector_num):\n",
        "    self.sector_num = sector_num\n",
        "    super(locals_context, self).__init__()\n",
        "    self.context_layers = [Context_Vector()\n",
        "                       for _ in range(sector_num)]   \n",
        "\n",
        "  def call(self, inputs):\n",
        "    inputs = self.preprocess(inputs)  #batch, sector_size, window_size, feature#\n",
        "\n",
        "    # LSTM input reshape #\n",
        "    # 기존 : batch, sector_size, window_size, feature\n",
        "    # 각 sector에 따라서, for문 반복\n",
        "    # 변경 : batch, window_size, feature\n",
        "    context_lstm = []\n",
        "    for sector_num in range(self.sector_num):\n",
        "      lstm_input1 = inputs[:,sector_num,:,:] # batch, window_size, feature\n",
        "\n",
        "      att_lstm = self.context_layers[sector_num](lstm_input1) # batch, 1, LSTM units\n",
        "      #att_lstm = tf.squeeze(att_lstm) # batch, LSTM units\n",
        "      context_lstm.append(att_lstm)\n",
        "\n",
        "    context = tf.stack(context_lstm)\n",
        "    context = tf.squeeze(context,2)\n",
        "    # temp shape =  sector_num, batch_size, LSTM units #\n",
        "    # 변경 -> temp shape = batch_size, sector_num, LSTM units # \n",
        "    after_att_lstm = tf.transpose(context, perm= [1, 0, 2]) # batch_size, sector_num, LSTM units # \n",
        "    return after_att_lstm\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def preprocess(self, inputs):\n",
        "    \n",
        "    #  데이터의 전처리 과정  #\n",
        "    # input shape = (batch, window_size, feature (concat 되어있음) ) \n",
        "    # output shape = (batch, sector_num, winodw_size, feature)\n",
        "    \n",
        "    test_input = inputs\n",
        "    thrid_sizes = []\n",
        "\n",
        "\n",
        "    batchs = inputs.shape[0]\n",
        "    windows = inputs.shape[1]\n",
        "    for batch in range(batchs):\n",
        "\n",
        "      start = 0 # feature의 시작점\n",
        "      end = 2 # feature의 종료점 \n",
        "      sec_sizes = []     \n",
        "      for num_of_sector in range(self.sector_num): # batch\n",
        "        temp = []\n",
        "        for i in range(windows):  \n",
        "          reshaped_data = test_input[batch][i][start:end]\n",
        "          reshaped_data = tf.reshape(reshaped_data, [1,2])\n",
        "          temp.append(reshaped_data)  \n",
        "        first_size = tf.stack(temp) # 50, 1, 2\n",
        "        first_size = tf.reshape(first_size, [windows,2])\n",
        "        sec_sizes.append(first_size)\n",
        "        start += 2\n",
        "        end += 2\n",
        "      print(\"*\", end = \"\" )\n",
        "      # 반복 batch만큼 반복시켜서, 새로운 데이터 shape을 만듬\n",
        "      #sec_sizes =  sector_size, window_size, feature\n",
        "      sec_sizes = tf.stack(sec_sizes)\n",
        "      thrid_sizes.append(sec_sizes)\n",
        "\n",
        "    thrid_sizes = tf.stack(thrid_sizes)\n",
        "    reshaped_output = thrid_sizes\n",
        "    return reshaped_output  #batch, sector_size, window_size, feature#\n",
        "\n",
        "#     2. multi_context를 사용한 상황 (입력 처음 sector가 global context로 사용)     #\n",
        "\n",
        "\n",
        "# class locals_context(tf.keras.Model):\n",
        "#   def __init__(self,sector_num, beta):\n",
        "#     self.sector_num = sector_num\n",
        "#     super(locals_context, self).__init__()\n",
        "#     self.context_layers = [Context_Vector()\n",
        "#                        for _ in range(sector_num)]   \n",
        "#     self.multi = tf.TensorArray(tf.float32, size = 0, dynamic_size=True)\n",
        "#     self.multis = tf.TensorArray(tf.float32, size = 0, dynamic_size=True)\n",
        "#     self.beta = beta\n",
        "\n",
        "\n",
        "#   def call(self, inputs):\n",
        "\n",
        "#     test_input = inputs\n",
        "\n",
        "#     batch_len = tf.shape(test_input)[0] # Batch_size\n",
        "    \n",
        "#     for batch in range(batch_len):\n",
        "#       start = 0 # 데이터의 간격\n",
        "#       end = 2\n",
        "#       for num_of_sector in range(self.sector_num):\n",
        "#         temp = tf.TensorArray(tf.float32, size = 0, dynamic_size=True)\n",
        "#         for i in range(50):  \n",
        "#           reshaped_data = test_input[batch][i][start:end]\n",
        "#           reshaped_data = tf.reshape(reshaped_data, [1,2])\n",
        "#           temp = temp.write(i, reshaped_data)\n",
        "          \n",
        "#         context_input = temp.stack() # 50,2\n",
        "#         context_input = tf.reshape(context_input, [1,50,2])\n",
        "\n",
        "#         if num_of_sector == 0:\n",
        "#           global_context = self.context_layers[num_of_sector](context_input)\n",
        "#           continue\n",
        "#         local_context = self.context_layers[num_of_sector](context_input)\n",
        "#         multi_context = local_context + self.beta * global_context\n",
        "        \n",
        "#         self.multi = self.multi.write(num_of_sector-1, multi_context) # multi_context의 경우\n",
        "\n",
        "#         start += 2\n",
        "#         end += 2\n",
        "#       output = tf.reshape(tensor = self.multi.stack(), shape = [self.sector_num-1,128]) # 1, sector-1 개 , 128\n",
        "#       self.multis = self.multis.write(batch, output) # BACTH_SIZE, OUTPUT\n",
        "#     output1 = self.multis.stack()\n",
        "#     return output1\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer): # \n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "\n",
        "    assert d_model % self.num_heads == 0\n",
        "\n",
        "    self.depth = d_model // self.num_heads\n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "  def split_heads(self, x, batch_size):\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "  def call(self, v, k, q):\n",
        "\n",
        "    batch_size = tf.shape(q)[0]\n",
        "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "\n",
        "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "        q, k, v)\n",
        "\n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "    concat_attention = tf.reshape(scaled_attention,\n",
        "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "    \n",
        "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "    \n",
        "    return output, attention_weights\n",
        "\n",
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "  ])\n",
        "\n",
        "def scaled_dot_product_attention(q, k, v): # transformer encoder 일부 수정\n",
        "  matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32) # d_model 차원을 알기위해\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk) # scaled 시킴\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "  output = tf.matmul(attention_weights, v)\n",
        "  return output, attention_weights\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):   # batch, sector_size, lstm_units#\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(axis = 1)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(axis = 1)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training):\n",
        "    \n",
        "    attn_output, _ = self.mha(x, x, x)  # batch, sector_size, lstm_units#\n",
        "    #---------------------------------------------------------------------#\n",
        "    attn_output = self.dropout1(attn_output,training = training)\n",
        "    out1 = self.layernorm1(x + attn_output)  #  batch, sector_size, lstm_units#\n",
        "    print(out1)\n",
        "    ffn_output = self.ffn(out1)  #  batch, sector_size, lstm_units#\n",
        "    ffn_output = self.dropout2(ffn_output,training = training) # training 할 때 배우기\n",
        "    out2 = self.layernorm2(out1 + ffn_output)  # batch, sector_size, lstm_units#\n",
        "    return out2 \n",
        "\n",
        "class Linear_Mapping(tf.keras.layers.Layer): #  batch, sector_size, lstm_units#\n",
        "  def __init__(self, units, sector_num):\n",
        "    super(Linear_Mapping, self).__init__()\n",
        "    self.sector_num = sector_num\n",
        "    self.W1 = tf.keras.layers.Dense(units, activation = 'relu')\n",
        "    self.dropout = tf.keras.layers.Dropout(0.2)\n",
        "    self.W2 = tf.keras.layers.Dense(20)\n",
        "    self.W3 = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, input, training):\n",
        "    batch = input.shape[0]\n",
        "    linear_output = self.W1(input)\n",
        "    dropout3 = self.dropout(linear_output, training = training)\n",
        "    linear_output = self.W2(dropout3)\n",
        "    linear_output = self.W3(linear_output)\n",
        "    return tf.reshape(linear_output, [batch, self.sector_num] ) # Batch size, sector_size\n",
        "\n",
        "class predict_value(tf.keras.Model):\n",
        "  def __init__(self, SECTOR_NUM ,LSTM_UNITS, NUM_HEAD, FF, LINEAR_UNIT):\n",
        "    super(predict_value, self).__init__()\n",
        "    self.localsa = locals_context(SECTOR_NUM) # sector_num( Global 포함)\n",
        "    self.encoder = EncoderLayer(LSTM_UNITS, NUM_HEAD, FF)\n",
        "    self.linear_mapping = Linear_Mapping(LINEAR_UNIT,SECTOR_NUM)\n",
        "\n",
        "  def call(self, inputs, training):\n",
        "    local = self.localsa(inputs)\n",
        "    encoder = self.encoder(local, training)\n",
        "    linear_out = self.linear_mapping(encoder, training)\n",
        "    return linear_out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvxB1Csgkqe_",
        "outputId": "9c28c6d7-91b9-41a2-82d4-e283eef03322"
      },
      "source": [
        "from tensorflow.keras.losses import Huber\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "sample_predict_value = predict_value(\n",
        "    SECTOR_NUM= 21,\n",
        "    LSTM_UNITS = 128, \n",
        "    NUM_HEAD = 4, \n",
        "    FF=128, \n",
        "    LINEAR_UNIT = 40,\n",
        "    )\n",
        "\n",
        "sample_predict_value(tf.keras.layers.Input(shape = (50,42), batch_size= 10))\n",
        "\n",
        "sample_predict_value.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**********Tensor(\"predict_value/encoder_layer/layer_normalization_21/batchnorm/add_1:0\", shape=(10, 21, 128), dtype=float32)\n",
            "Model: \"predict_value\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " locals_context (locals_cont  multiple                 1414014   \n",
            " ext)                                                            \n",
            "                                                                 \n",
            " encoder_layer (EncoderLayer  multiple                 99156     \n",
            " )                                                               \n",
            "                                                                 \n",
            " linear__mapping (Linear_Map  multiple                 6001      \n",
            " ping)                                                           \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,519,171\n",
            "Trainable params: 1,519,171\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kg5kG5BPvlFo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e19f0ff1-754d-46b3-8344-01eeef5edf21"
      },
      "source": [
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(0.001) # 0.005, 0.001, 0.0005, 0.0001 \n",
        "loss_object = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "checkpoint_path = \"/content/drive/MyDrive/NewVersion/checkpoint3.ckpt\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(model = sample_predict_value,\n",
        "                            optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep= 1)\n",
        "\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def train_step(images, labels):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = sample_predict_value(images, training=True)\n",
        "    loss = loss_object(labels, predictions)\n",
        "  gradients = tape.gradient(loss, sample_predict_value.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, sample_predict_value.trainable_variables))\n",
        "\n",
        "  train_loss(loss)\n",
        "\n",
        "@tf.function\n",
        "def test_step(images, labels):\n",
        "  predictions = sample_predict_value(images, training=False)\n",
        "  t_loss = loss_object(labels, predictions)\n",
        "\n",
        "  test_loss(t_loss)\n",
        "\n",
        "EPOCHS = 150\n",
        "\n",
        "min = 1\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  # Reset the metrics at the start of the next epoch\n",
        "  train_loss.reset_states()\n",
        "\n",
        "  for train_x, train_y in train_data:\n",
        "    if train_x.shape[0] != BATCH_SIZE:\n",
        "      continue\n",
        "    train_step(train_x, train_y)\n",
        "\n",
        "  for valid_x, valid_y in valid_data:\n",
        "    if valid_x.shape[0] != BATCH_SIZE:\n",
        "      continue\n",
        "    test_step(valid_x, valid_y)\n",
        "\n",
        "  print( f'Epoch {epoch + 1}, '\n",
        "  f'Train Loss: {train_loss.result()}, '\n",
        "  f'validation Loss: {test_loss.result()},\\n'\n",
        "  )\n",
        "\n",
        "  # validaiton loss value is the lowest\n",
        "  if test_loss.result() < min:\n",
        "    min = test_loss.result()\n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print(\"Validation Loss is better than Train Loss. so New parameter is stored drive \\n\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**********Tensor(\"predict_value/encoder_layer/layer_normalization_21/batchnorm/add_1:0\", shape=(10, 21, 128), dtype=float32)\n",
            "**********Tensor(\"predict_value/encoder_layer/layer_normalization_21/batchnorm/add_1:0\", shape=(10, 21, 128), dtype=float32)\n",
            "**********Tensor(\"predict_value/encoder_layer/layer_normalization_21/batchnorm/add_1:0\", shape=(10, 21, 128), dtype=float32)\n",
            "Epoch 1, Train Loss: 0.3207542598247528, validation Loss: 0.04008394479751587,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 2, Train Loss: 0.07072760909795761, validation Loss: 0.03756430000066757,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 3, Train Loss: 0.0537097342312336, validation Loss: 0.03639240562915802,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 4, Train Loss: 0.047792453318834305, validation Loss: 0.034931693226099014,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 5, Train Loss: 0.04248083755373955, validation Loss: 0.03431490808725357,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 6, Train Loss: 0.040419138967990875, validation Loss: 0.033701732754707336,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 7, Train Loss: 0.03838029131293297, validation Loss: 0.03359459340572357,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 8, Train Loss: 0.03748886659741402, validation Loss: 0.03306741639971733,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 9, Train Loss: 0.036145515739917755, validation Loss: 0.032607730478048325,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 10, Train Loss: 0.035210076719522476, validation Loss: 0.03229227289557457,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 11, Train Loss: 0.034619852900505066, validation Loss: 0.0321122407913208,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 12, Train Loss: 0.03458631411194801, validation Loss: 0.03183768689632416,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 13, Train Loss: 0.03344147279858589, validation Loss: 0.03160948306322098,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 14, Train Loss: 0.033483706414699554, validation Loss: 0.031387828290462494,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 15, Train Loss: 0.033201973885297775, validation Loss: 0.03122861310839653,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 16, Train Loss: 0.03303754702210426, validation Loss: 0.031056294217705727,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 17, Train Loss: 0.033105023205280304, validation Loss: 0.03091701678931713,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 18, Train Loss: 0.03279762342572212, validation Loss: 0.030803637579083443,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 19, Train Loss: 0.03275655582547188, validation Loss: 0.03069627843797207,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 20, Train Loss: 0.032550182193517685, validation Loss: 0.030594000592827797,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 21, Train Loss: 0.03235680237412453, validation Loss: 0.030573882162570953,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 22, Train Loss: 0.03241290524601936, validation Loss: 0.03052816167473793,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 23, Train Loss: 0.0325908288359642, validation Loss: 0.03042750619351864,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 24, Train Loss: 0.03227950260043144, validation Loss: 0.03039686009287834,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 25, Train Loss: 0.03244708478450775, validation Loss: 0.0303146094083786,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 26, Train Loss: 0.032169047743082047, validation Loss: 0.0302524883300066,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 27, Train Loss: 0.03227120265364647, validation Loss: 0.030179886147379875,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 28, Train Loss: 0.032242242246866226, validation Loss: 0.03013848513364792,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 29, Train Loss: 0.03221266344189644, validation Loss: 0.030095186084508896,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 30, Train Loss: 0.03247645124793053, validation Loss: 0.030033588409423828,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 31, Train Loss: 0.03194984793663025, validation Loss: 0.029983198270201683,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 32, Train Loss: 0.031942158937454224, validation Loss: 0.0299328975379467,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 33, Train Loss: 0.03167684003710747, validation Loss: 0.029901063069701195,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 34, Train Loss: 0.03206587955355644, validation Loss: 0.02985900454223156,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 35, Train Loss: 0.032140348106622696, validation Loss: 0.02982190251350403,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 36, Train Loss: 0.03225092962384224, validation Loss: 0.02978634089231491,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 37, Train Loss: 0.032151561230421066, validation Loss: 0.029746055603027344,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 38, Train Loss: 0.03206151723861694, validation Loss: 0.02974039874970913,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 39, Train Loss: 0.032024312764406204, validation Loss: 0.029715489596128464,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 40, Train Loss: 0.03237447515130043, validation Loss: 0.029685692861676216,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 41, Train Loss: 0.03227176144719124, validation Loss: 0.029661068692803383,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 42, Train Loss: 0.03230569511651993, validation Loss: 0.029628973454236984,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 43, Train Loss: 0.03211165592074394, validation Loss: 0.0295988991856575,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 44, Train Loss: 0.0319778248667717, validation Loss: 0.029572593048214912,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 45, Train Loss: 0.03220885246992111, validation Loss: 0.029545629397034645,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 46, Train Loss: 0.03254854306578636, validation Loss: 0.029555784538388252,\n",
            "\n",
            "Epoch 47, Train Loss: 0.03237743675708771, validation Loss: 0.02953164651989937,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 48, Train Loss: 0.0322226881980896, validation Loss: 0.029507476836442947,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 49, Train Loss: 0.032002050429582596, validation Loss: 0.029484957456588745,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 50, Train Loss: 0.032219018787145615, validation Loss: 0.02947082556784153,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 51, Train Loss: 0.032048892229795456, validation Loss: 0.0294756181538105,\n",
            "\n",
            "Epoch 52, Train Loss: 0.032056327909231186, validation Loss: 0.02945505641400814,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 53, Train Loss: 0.03258378058671951, validation Loss: 0.029437590390443802,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 54, Train Loss: 0.03218940272927284, validation Loss: 0.029418544843792915,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 55, Train Loss: 0.032056327909231186, validation Loss: 0.029399236664175987,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 56, Train Loss: 0.03236301988363266, validation Loss: 0.029383407905697823,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 57, Train Loss: 0.032188381999731064, validation Loss: 0.029379941523075104,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 58, Train Loss: 0.03202922269701958, validation Loss: 0.029368730261921883,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 59, Train Loss: 0.03204226866364479, validation Loss: 0.029365012422204018,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 60, Train Loss: 0.03220321983098984, validation Loss: 0.029353542253375053,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 61, Train Loss: 0.03219091519713402, validation Loss: 0.029339687898755074,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 62, Train Loss: 0.032494764775037766, validation Loss: 0.029324568808078766,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 63, Train Loss: 0.03242887184023857, validation Loss: 0.029315784573554993,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 64, Train Loss: 0.032118119299411774, validation Loss: 0.02930779755115509,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 65, Train Loss: 0.03230396658182144, validation Loss: 0.02929326333105564,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 66, Train Loss: 0.031925786286592484, validation Loss: 0.029280483722686768,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 67, Train Loss: 0.03239116817712784, validation Loss: 0.029269957914948463,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 68, Train Loss: 0.032399941235780716, validation Loss: 0.02925623394548893,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 69, Train Loss: 0.03221847862005234, validation Loss: 0.02924974448978901,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 70, Train Loss: 0.03205760940909386, validation Loss: 0.02924131602048874,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 71, Train Loss: 0.03245330601930618, validation Loss: 0.029237929731607437,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 72, Train Loss: 0.03192436322569847, validation Loss: 0.029224609956145287,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 73, Train Loss: 0.032084204256534576, validation Loss: 0.029224375262856483,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 74, Train Loss: 0.03212648257613182, validation Loss: 0.029216334223747253,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 75, Train Loss: 0.032551299780607224, validation Loss: 0.029207056388258934,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 76, Train Loss: 0.03214827552437782, validation Loss: 0.029202502220869064,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 77, Train Loss: 0.032209012657403946, validation Loss: 0.029198549687862396,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 78, Train Loss: 0.032266728579998016, validation Loss: 0.029194118455052376,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 79, Train Loss: 0.032285138964653015, validation Loss: 0.029181748628616333,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 80, Train Loss: 0.03240896388888359, validation Loss: 0.02917218953371048,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 81, Train Loss: 0.03241659328341484, validation Loss: 0.02916109748184681,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 82, Train Loss: 0.0320613794028759, validation Loss: 0.029157305136322975,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 83, Train Loss: 0.03213333711028099, validation Loss: 0.02915184572339058,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 84, Train Loss: 0.03218506649136543, validation Loss: 0.029149046167731285,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 85, Train Loss: 0.032092612236738205, validation Loss: 0.02914411574602127,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 86, Train Loss: 0.032134946435689926, validation Loss: 0.029134659096598625,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 87, Train Loss: 0.03208104521036148, validation Loss: 0.029127532616257668,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 88, Train Loss: 0.032533612102270126, validation Loss: 0.029119422659277916,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 89, Train Loss: 0.03229851648211479, validation Loss: 0.02911064215004444,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 90, Train Loss: 0.03203785791993141, validation Loss: 0.029104843735694885,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 91, Train Loss: 0.0320565328001976, validation Loss: 0.029098043218255043,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 92, Train Loss: 0.032103944569826126, validation Loss: 0.029092663899064064,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 93, Train Loss: 0.03233976662158966, validation Loss: 0.029086392372846603,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 94, Train Loss: 0.03211623430252075, validation Loss: 0.029082056134939194,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 95, Train Loss: 0.031884703785181046, validation Loss: 0.029087519273161888,\n",
            "\n",
            "Epoch 96, Train Loss: 0.03231312707066536, validation Loss: 0.029082495719194412,\n",
            "\n",
            "Epoch 97, Train Loss: 0.03211141377687454, validation Loss: 0.029081877321004868,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 98, Train Loss: 0.032009415328502655, validation Loss: 0.02907327376306057,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 99, Train Loss: 0.032074540853500366, validation Loss: 0.02906924858689308,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 100, Train Loss: 0.03209950774908066, validation Loss: 0.02906518615782261,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 101, Train Loss: 0.03205928951501846, validation Loss: 0.0290607288479805,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 102, Train Loss: 0.032015442848205566, validation Loss: 0.02905353158712387,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 103, Train Loss: 0.03204204514622688, validation Loss: 0.029050160199403763,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 104, Train Loss: 0.032400552183389664, validation Loss: 0.02905072458088398,\n",
            "\n",
            "Epoch 105, Train Loss: 0.03214459866285324, validation Loss: 0.029045427218079567,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 106, Train Loss: 0.03225312754511833, validation Loss: 0.029041897505521774,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 107, Train Loss: 0.03208988159894943, validation Loss: 0.029035216197371483,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 108, Train Loss: 0.032567348331213, validation Loss: 0.029032111167907715,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 109, Train Loss: 0.03200932592153549, validation Loss: 0.029025288298726082,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 110, Train Loss: 0.03223692253232002, validation Loss: 0.029019497334957123,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 111, Train Loss: 0.03216579928994179, validation Loss: 0.029015731066465378,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 112, Train Loss: 0.03204317390918732, validation Loss: 0.029011426493525505,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 113, Train Loss: 0.03183456137776375, validation Loss: 0.02900628373026848,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 114, Train Loss: 0.0323999859392643, validation Loss: 0.02900206297636032,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 115, Train Loss: 0.03233092650771141, validation Loss: 0.029012689366936684,\n",
            "\n",
            "Epoch 116, Train Loss: 0.03198378533124924, validation Loss: 0.029007667675614357,\n",
            "\n",
            "Epoch 117, Train Loss: 0.03209079056978226, validation Loss: 0.029001079499721527,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 118, Train Loss: 0.03199365735054016, validation Loss: 0.029001731425523758,\n",
            "\n",
            "Epoch 119, Train Loss: 0.03208426758646965, validation Loss: 0.028999505564570427,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 120, Train Loss: 0.03201857954263687, validation Loss: 0.028995443135499954,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 121, Train Loss: 0.03230256214737892, validation Loss: 0.028989745303988457,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 122, Train Loss: 0.03212115541100502, validation Loss: 0.028992492705583572,\n",
            "\n",
            "Epoch 123, Train Loss: 0.03198525682091713, validation Loss: 0.028986886143684387,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 124, Train Loss: 0.031986888498067856, validation Loss: 0.028981193900108337,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 125, Train Loss: 0.03194660320878029, validation Loss: 0.028978314250707626,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 126, Train Loss: 0.0320240780711174, validation Loss: 0.02897373028099537,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 127, Train Loss: 0.03211844339966774, validation Loss: 0.02897470071911812,\n",
            "\n",
            "Epoch 128, Train Loss: 0.03197052329778671, validation Loss: 0.028971228748559952,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 129, Train Loss: 0.031846415251493454, validation Loss: 0.028973545879125595,\n",
            "\n",
            "Epoch 130, Train Loss: 0.032015111297369, validation Loss: 0.02896862104535103,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 131, Train Loss: 0.03205796703696251, validation Loss: 0.028965389356017113,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 132, Train Loss: 0.03203641623258591, validation Loss: 0.028968563303351402,\n",
            "\n",
            "Epoch 133, Train Loss: 0.032487038522958755, validation Loss: 0.028964990749955177,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 134, Train Loss: 0.03200802952051163, validation Loss: 0.028970884159207344,\n",
            "\n",
            "Epoch 135, Train Loss: 0.03239334747195244, validation Loss: 0.028966736048460007,\n",
            "\n",
            "Epoch 136, Train Loss: 0.032085780054330826, validation Loss: 0.028961457312107086,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 137, Train Loss: 0.0322534404695034, validation Loss: 0.028957344591617584,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 138, Train Loss: 0.03208000212907791, validation Loss: 0.028955871239304543,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 139, Train Loss: 0.03211117163300514, validation Loss: 0.02895328961312771,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 140, Train Loss: 0.03219182416796684, validation Loss: 0.02895037829875946,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 141, Train Loss: 0.03199394792318344, validation Loss: 0.028950519859790802,\n",
            "\n",
            "Epoch 142, Train Loss: 0.03219682723283768, validation Loss: 0.028948083519935608,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 143, Train Loss: 0.03211882710456848, validation Loss: 0.02894493006169796,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 144, Train Loss: 0.03208111599087715, validation Loss: 0.028940808027982712,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 145, Train Loss: 0.031975843012332916, validation Loss: 0.0289361160248518,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 146, Train Loss: 0.03225746005773544, validation Loss: 0.028936514630913734,\n",
            "\n",
            "Epoch 147, Train Loss: 0.03200285881757736, validation Loss: 0.028932688757777214,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 148, Train Loss: 0.032146669924259186, validation Loss: 0.028931105509400368,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 149, Train Loss: 0.031943269073963165, validation Loss: 0.028928197920322418,\n",
            "\n",
            "Validation Loss is better than Train Loss. so New parameter is stored drive \n",
            "\n",
            "Epoch 150, Train Loss: 0.031988415867090225, validation Loss: 0.02892843447625637,\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8KsOKBQynTa",
        "outputId": "9e99d48b-8f8b-43fa-e6f2-5bab75de4c3f"
      },
      "source": [
        "test_input = df_[-51:-1]\n",
        "test_input = np.expand_dims(test_input,0)\n",
        "\n",
        "results = sample_predict_value(test_input, training = False)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "   ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "   print('Latest checkpoint restored!!')\n",
        "\n",
        "final_result = []\n",
        "\n",
        "sector_num = 21\n",
        "for i in range(sector_num):\n",
        "  value = results[0][i] * (max_value[i] - min_value[i]) + min_value[i]\n",
        "  final_result.append(value)\n",
        "final_result = tf.stack(final_result)\n",
        "\n",
        "print(\"\\n21가지의 농산물 가격 출력 : \")\n",
        "print(final_result)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*tf.Tensor(\n",
            "[[[ 9.5077306e-01 -4.2049289e-02  1.5824772e-01 ... -1.2339503e-03\n",
            "    1.9781667e-01 -1.2552534e-01]\n",
            "  [ 3.2637286e-01 -1.7531331e-01 -8.4314696e-02 ... -3.7118942e-01\n",
            "   -1.8052176e-01 -3.8221294e-01]\n",
            "  [-6.7662132e-01 -2.2568813e-01  3.9455861e-02 ...  1.5022025e-01\n",
            "   -1.2660500e-01 -6.8805826e-01]\n",
            "  ...\n",
            "  [-1.5165542e-01  2.9242006e-01  6.0833389e-01 ... -5.6306183e-02\n",
            "    1.7253625e-01  2.9382354e-01]\n",
            "  [-3.4020257e-01 -5.2597690e-01  1.3681761e+00 ... -4.9871132e-02\n",
            "    3.7098601e-01  9.4004834e-01]\n",
            "  [-5.7968831e-01  1.8478721e-01  2.0294459e-01 ...  1.7208068e-01\n",
            "   -1.2573123e-02 -9.1147497e-02]]], shape=(1, 21, 128), dtype=float32)\n",
            "Latest checkpoint restored!!\n",
            "\n",
            "21가지의 농산물 가격 출력 : \n",
            "tf.Tensor(\n",
            "[  716.52045   567.275     676.8801  29093.34     3771.6755   1291.8198\n",
            "  1142.1919    607.65216  5572.424    3172.8826   2751.5264   1057.5107\n",
            "  3259.2314   2139.0303   1729.8989   2235.5776   4077.2935   1908.4614\n",
            "  1786.4315   4062.7415  12850.806  ], shape=(21,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MsboQyUQ9XP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}